# ğŸŒ Concurrent Web Crawler (TypeScript + Node.js)

A **concurrent web crawler** built with **TypeScript, Node.js, and p-limit** to control concurrency.  
It crawls a given website, fetches internal links recursively, and generates a report of how many times each page was discovered.

---

## âœ¨ Features
- ğŸŒ Crawl any website starting from a base URL
- âš¡ Concurrency control with [`p-limit`](https://github.com/sindresorhus/p-limit)  
- ğŸ”— Extracts links from HTML using [JSDOM](https://github.com/jsdom/jsdom)  
- ğŸ“Š Generates a **report** showing internal link counts
- ğŸ›¡ï¸ Error handling for invalid links, non-HTML content, and failed requests

- ğŸ“¦ Tech Stack
TypeScript
Node.js
JSDOM â†’ HTML parsing
p-limit â†’ Concurrency control

---

## ğŸš€ Getting Started

### 1. Clone the repository
```bash
git clone https://github.com/<your-username>/typescript-crawler.git
cd typescript-crawler
```


### 2. Install dependencies
```bash
npm install
```


### 3. Run the crawler
```bash
npm run start https://example.com
```

### Output example:
```pgsql
=============================
  REPORT for https://example.com
=============================
Found 5 links to https://example.com
Found 3 links to https://example.com/about
Found 2 links to https://example.com/contact
```
